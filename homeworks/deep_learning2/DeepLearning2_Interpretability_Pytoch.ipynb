{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretability of Deep Learning: Estimating importance scores\n",
        "### 1000-719bMSB MIM UW, Neo Christopher Chung\n",
        "\n",
        "In this lab, we estimate importance scores using backpropagation, which is one of the first XAI methods. There are many names for scores that relate the input features to the output class. Saliency maps, feature attribution or importance scores all refer to the very closely related, if not the same, approach.\n",
        "\n",
        "In the process, we also learn how to use a pre-trained model, called SqueezeNet (AlexNet-level accuracy with 50x fewer parameters and 0.5MB model size), which can be loaded directly from PyTorch.  \n",
        "https://arxiv.org/abs/1602.07360\n",
        "https://en.wikipedia.org/wiki/SqueezeNet\n",
        "\n",
        "We further look at the ImageNet which is one of the most popular and important database consisted of milliions of images across 20000 categories. For Colab, we use only a small portion of the ImageNet\n",
        "https://ieeexplore.ieee.org/document/5206848\n",
        "https://en.wikipedia.org/wiki/ImageNet\n",
        "\n",
        "Using these ingredients, we calculate backpropgagtion based importance scores from scratch.\n",
        "\n",
        "Please be mindful of both original (multi-channel) values and summaried 2D values. Both are used and researched in practice.\n",
        "\n",
        "Adapted from https://github.com/srinadhu/CS231n/blob/master/assignment3/NetworkVisualization-PyTorch.ipynb\n"
      ],
      "metadata": {
        "id": "8tAvXxnAlAMP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pnx4TO5WYmOz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "461816bf-cc93-43af-d889-4125abb85814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-f480a97f4b63>:7: DeprecationWarning: Please import `gaussian_filter1d` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import gaussian_filter1d\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f480a97f4b63>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# only if you are running this from google colab an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#sample ImageNet data from https://github.com/CNN-ADF/Task2020\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from matplotlib import cm\n",
        "# configuration for visualizing with\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "SQUEEZENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "SQUEEZENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "\n",
        "# only if you are running this from google colab an\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#sample ImageNet data from https://github.com/CNN-ADF/Task2020\n",
        "!wget 'https://raw.githubusercontent.com/CNN-ADF/Task2020/master/resources/imagenet_val_25.npz' -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions for image processing\n",
        "def preprocess(img, size=224):\n",
        "    transform = T.Compose([\n",
        "        T.Resize(size),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
        "                    std=SQUEEZENET_STD.tolist()),\n",
        "        T.Lambda(lambda x: x[None]),\n",
        "    ])\n",
        "    return transform(img)\n",
        "\n",
        "def rescale(x):\n",
        "    low, high = x.min(), x.max()\n",
        "    x_rescaled = (x - low) / (high - low)\n",
        "    return x_rescaled\n",
        "\n",
        "def deprocess(img, should_rescale=True):\n",
        "    transform = T.Compose([\n",
        "        T.Lambda(lambda x: x[0]),\n",
        "        T.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),\n",
        "        T.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),\n",
        "        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n",
        "        T.ToPILImage(),\n",
        "    ])\n",
        "    return transform(img)\n",
        "\n",
        "def blur_image(X, sigma=1):\n",
        "    X_np = X.cpu().clone().numpy()\n",
        "    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n",
        "    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n",
        "    X.copy_(torch.Tensor(X_np).type_as(X))\n",
        "    return X\n",
        "\n",
        "# load small imagenet data\n",
        "def load_imagenet_val(num=None):\n",
        "    f = np.load('imagenet_val_25.npz', allow_pickle=True)\n",
        "    X = f['X']\n",
        "    y = f['y']\n",
        "    class_names = f['label_map'].item()\n",
        "    idx = np.arange(25)\n",
        "    np.random.shuffle(idx)\n",
        "    if num is not None:\n",
        "        idx = idx[:num]\n",
        "        X   = X[idx]\n",
        "        y   = y[idx]\n",
        "    return X, y, class_names\n",
        "\n",
        "#X, y, class_names = load_imagenet_val(num=5)\n",
        "\n",
        "#Load and use all 25 images from a smaller set, downloaded\n",
        "f = np.load('imagenet_val_25.npz', allow_pickle=True)\n",
        "X = f['X']\n",
        "y = f['y']\n",
        "class_names = f['label_map'].item()\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "Z23j-gFmYu-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check out which number relates to what class names\n",
        "for y_val in y:\n",
        "    print(class_names[y_val])"
      ],
      "metadata": {
        "id": "YZL4ygP-B0UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show some images\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(5):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(X[i])\n",
        "    plt.title(class_names[y[i]])\n",
        "    plt.axis('off')\n",
        "plt.gcf().tight_layout()"
      ],
      "metadata": {
        "id": "bMHAJbgvZpky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and 0.5MB model size\n",
        "https://arxiv.org/abs/1602.07360\n",
        "\n",
        "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n",
        "\n",
        "https://github.com/forresti/SqueezeNet"
      ],
      "metadata": {
        "id": "Kp_Glwt9tds2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iandola et al, \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size\", arXiv 2016\n",
        "model = torchvision.models.squeezenet1_1(pretrained=True)\n",
        "#print(model)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "8h8CpN8nZxsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n",
        "y_tensor = torch.LongTensor(y)\n",
        "model.eval()\n",
        "scores = model(X_tensor)\n",
        "print(scores)\n",
        "scores_y = scores.gather(1, y_tensor.view(-1, 1)).squeeze()\n",
        "print(scores_y)"
      ],
      "metadata": {
        "id": "VTixGQEAY2d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_saliency_maps(X, y, model):\n",
        "    \"\"\"\n",
        "    Compute a class saliency map using the model for images X and labels y.\n",
        "\n",
        "    Input:\n",
        "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
        "    - y: Labels for X; LongTensor of shape (N,)\n",
        "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
        "\n",
        "    Returns:\n",
        "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
        "    images.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    X.requires_grad_()\n",
        "\n",
        "    # 1. Forward pass\n",
        "    scores = model(X)\n",
        "\n",
        "    # 2. Get correct class scores\n",
        "    scores = scores.gather(1, y.view(-1, 1)).squeeze()\n",
        "    print(\"== class scores ==\")\n",
        "    print(scores)\n",
        "\n",
        "    # 3. Backward pass\n",
        "    scores_size = scores.shape\n",
        "    ones_tensor = torch.ones(scores_size)\n",
        "    scores.backward(ones_tensor)\n",
        "\n",
        "    # 4. retrieve the gradient as saliency map\n",
        "    saliency = X.grad\n",
        "    return saliency\n",
        "\n",
        "def compute_abs(saliency):\n",
        "    saliency_abs = saliency.abs()\n",
        "    return saliency_abs\n",
        "\n",
        "def compute_max(saliency):\n",
        "    saliency_max, _= torch.max(saliency, dim=1)\n",
        "    return saliency_max"
      ],
      "metadata": {
        "id": "gEKptYyca8NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## calculating gradients for CORRECT labels\n",
        "# Convert X and y from numpy arrays to Torch Tensors\n",
        "X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n",
        "y_tensor = torch.LongTensor(y)\n",
        "\n",
        "# Compute saliency maps for images in X\n",
        "saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n",
        "print(saliency.shape)\n",
        "\n",
        "# Convert the saliency map from Torch Tensor to numpy array and show images\n",
        "# and saliency maps together.\n",
        "#saliency = saliency.numpy()\n"
      ],
      "metadata": {
        "id": "xtUahbhCdUHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking max or max-abs values are typical in the field\n",
        "saliency_max = compute_max(saliency)\n",
        "saliency_maxabs = compute_max(compute_abs(saliency))\n",
        "\n",
        "# show a chosen image and saliency map\n",
        "i=2\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(X[i])\n",
        "plt.title(class_names[y[i]])\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(saliency_maxabs[i,:,:])\n",
        "plt.title(class_names[y[i]])\n",
        "plt.axis('off')\n",
        "\n",
        "plt.gcf().tight_layout()\n"
      ],
      "metadata": {
        "id": "ZK-Qo3mYAVA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one could make a different color palette (see cmap)\n",
        "# https://matplotlib.org/stable/users/explain/colors/colormaps.html\n",
        "\n",
        "# even more control available\n",
        "# hue_neg, hue_pos = 0, 359\n",
        "# cmap = sns.diverging_palette(hue_neg, hue_pos, s=100, center=\"dark\", as_cmap=True)\n",
        "\n",
        "# show a chosen image and saliency map\n",
        "i=2\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(X[i])\n",
        "plt.title(class_names[y[i]])\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(saliency_maxabs[i,:,:], cmap=plt.cm.hot)\n",
        "plt.title(class_names[y[i]])\n",
        "plt.axis('off')\n",
        "\n",
        "plt.gcf().tight_layout()"
      ],
      "metadata": {
        "id": "nodYJxvV0nV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the actual values. we call these numbers importance scores\n",
        "saliency_max[i,:,:].numpy()"
      ],
      "metadata": {
        "id": "gz-lPwlu0eP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot multiple -- Note that you need to make a figure (5 samples) just like this in the homework, except you use SmoothGrad.\n",
        "N = 5\n",
        "for i in range(N):\n",
        "    plt.subplot(2, N, i + 1)\n",
        "    plt.imshow(X[i])\n",
        "    plt.axis('off')\n",
        "    plt.title(class_names[y[i]])\n",
        "    plt.subplot(2, N, N + i + 1)\n",
        "    plt.imshow(saliency_maxabs[i].numpy(), cmap=plt.cm.hot)\n",
        "    plt.axis('off')\n",
        "    plt.gcf().set_size_inches(12, 5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RM9LHkP2uabV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the historgram of the importance scores (raw saliency map values)\n",
        "plt.hist(saliency.numpy().flatten(), density=True, bins=1000)\n",
        "plt.xlim([-.2,0.2])"
      ],
      "metadata": {
        "id": "UyeJ2om1BRMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the historgram of max-abs importance scores\n",
        "plt.hist(saliency_maxabs.numpy().flatten(), density=True, bins=1000)\n",
        "plt.xlim([0,0.25])"
      ],
      "metadata": {
        "id": "1k_kBBVGzAl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "biXumSI6Hov7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SmoothGrad\n",
        "\n",
        "> Smilkov et al. (2017) “SmoothGrad: removing noise by adding noise”.\n",
        "> The core idea is to take an image of interest, sample similar images by adding noise to the image, then take the average of the resulting sensitivity (saliency) maps for each sampled image.\n",
        "\n",
        "Let's start building SmoothGrad."
      ],
      "metadata": {
        "id": "7PB9NusWd_j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to add a noise to an image\n",
        "def add_noise(x, noise_pct=0.05):\n",
        "    # Calculate the noise level\n",
        "    noise_level = noise_pct * np.std(x)\n",
        "    noise = np.random.normal(0, noise_level, size=x.shape)\n",
        "\n",
        "    # Add the noise to the sample\n",
        "    noisy_sample = x + noise\n",
        "\n",
        "    # Clip the values to ensure they remain within the valid range (0-255 for uint8 images)\n",
        "    noisy_sample = np.clip(noisy_sample, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return noisy_sample\n",
        "\n",
        "# Example\n",
        "i = 2\n",
        "sample = X[i]\n",
        "print(np.std(sample))\n",
        "\n",
        "noisy_sample = add_noise(x=sample, noise_pct=0.05)\n",
        "\n",
        "# Visualize the noise-added sample\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(noisy_sample)\n",
        "plt.title(class_names[y[i]] + \" + noise\")\n",
        "plt.axis('off')\n"
      ],
      "metadata": {
        "id": "CeOQ8da1IHf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_tensor"
      ],
      "metadata": {
        "id": "xlXGwU1mMOqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute saliency map from a noisy image\n",
        "\n",
        "# numpy array must be converted to a PyTorch tensor and processed using the same preprocess function\n",
        "noisy_sample_tensor = torch.tensor(noisy_sample, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
        "noisy_sample_tensor = preprocess(Image.fromarray(noisy_sample))\n",
        "\n",
        "saliency = compute_saliency_maps(noisy_sample_tensor, y_tensor[i].unsqueeze(0), model)\n",
        "saliency_max = compute_max(saliency)\n",
        "saliency_maxabs = compute_max(compute_abs(saliency))\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(X[i])\n",
        "plt.title(class_names[y[i]])\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(saliency_maxabs[0,:,:].detach().numpy(), cmap=plt.cm.hot)\n",
        "plt.title(class_names[y[i]])\n",
        "plt.axis('off')\n",
        "\n",
        "plt.gcf().tight_layout()\n"
      ],
      "metadata": {
        "id": "Lkp3GXPVMFSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework\n",
        "\n",
        "Make a function to create SmoothGrad, where the input arguments are X, y, model, n, and noise_pct. For simplicity, we only consider max-abs values. The below is the step for the SmoothGrad function in details:\n",
        "\n",
        "1. Ues add_noise to add noise (controlled by noise_pct) to a sample\n",
        "2. The noisy sample is processed through compute_saliency_maps, where saliency_maxabs is saved. This process is repeat *n* times.\n",
        "3. Take and return the avergeof *n* saliency_maxabs arrays.\n",
        "\n",
        "Visualize the first five images and their SmoothGrad heatmaps. See and compare that with the figure above using (vanilla) salincy maps.\n",
        "\n",
        "Please submit the notebook and the PDF/PNG image of these five images and their SmoothGrad heatmaps."
      ],
      "metadata": {
        "id": "MGAqznxUe1QP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQQhtWjrH02V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}